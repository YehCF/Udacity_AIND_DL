{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Analyzing IMDB Data in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import Dependencies\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1. Loading the data\n",
    "This dataset comes preloaded with Keras, so one simple command will get us training and testing data. There is a parameter for how many words we want to look at. We've set it at 1000, but feel free to experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "# Lading the data (it's preloaded in Keras)\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=1000)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2. Examining the data\n",
    "Notice that the data has been already pre-processed, where all the words have numbers, and the reviews come in as a vector with the words that the review contains. For example, if the word 'the' is the first one in our dictionary, and a review contains the word 'the', then there is a 1 in the corresponding vector.\n",
    "\n",
    "The output comes as a vector of 1's and 0's, where 1 is a positive sentiment for the review, and 0 is negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 2, 2, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 2, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4, 2, 2, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 2, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2, 2, 16, 480, 66, 2, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 2, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 2, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 2, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3. One-hot encoding the output\n",
    "Here, we'll turn the input vectors into (0,1)-vectors. For example, if the pre-processed vector contains the number 14, then in the processed vector, the 14th entry will be 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: (25000, 1000)\n",
      "[ 0.  1.  1.  0.  1.  1.  1.  1.  1.  1.  0.  0.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  0.  1.  1.  0.  0.  1.  1.  0.  1.  0.  1.  0.  1.  1.  0.  1.\n",
      "  1.  0.  1.  1.  0.  0.  0.  1.  0.  0.  1.  0.  1.  0.  1.  1.  1.  0.\n",
      "  0.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  1.  1.  0.  0.  0.  0.  1.\n",
      "  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.  1.  0.\n",
      "  0.  0.  1.  0.  0.  0.  0.  0.  1.  0.  1.  0.  0.  1.  1.  0.  1.  1.\n",
      "  0.  0.  0.  0.  1.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "  0.  0.  0.  0.  1.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  1.  0.  0.\n",
      "  1.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  1.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  1.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# One-hot encoding the output into vector mode, each of length 1000\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
    "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
    "print(\"Shape of x_train:\",x_train.shape)\n",
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "And we'll also one-hot encode the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 2)\n",
      "(25000, 2)\n"
     ]
    }
   ],
   "source": [
    "# One-hot encoding the output\n",
    "num_classes = 2\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 4. Building the  model architecture\n",
    "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_96 (Dense)             (None, 512)               512512    \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_97 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_98 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_99 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_100 (Dense)            (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 685,122.0\n",
      "Trainable params: 685,122.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "# Build the model architecture\n",
    "model = Sequential()\n",
    "#model.add(Dense(2048, input_shape=(1000,)))\n",
    "#model.add(Activation(\"relu\"))\n",
    "#model.add(Dense(1024, kernel_regularizer=regularizers.l2(1e-4)))\n",
    "#model.add(Activation(\"relu\"))\n",
    "model.add(Dense(512, input_shape=(1000,), kernel_regularizer=regularizers.l2(1e-2)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(256, kernel_regularizer=regularizers.l2(1e-2)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(128, kernel_regularizer=regularizers.l2(1e-2)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(64, kernel_regularizer=regularizers.l2(1e-2)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(2))\n",
    "\n",
    "# Compile the model using a loss function and an optimizer.\n",
    "opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 5. Training the model\n",
    "Run the model here. Experiment with different batch_size, and number of epochs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/30\n",
      "10s - loss: 4.4157 - acc: 0.7369 - val_loss: 2.5560 - val_acc: 0.8174\n",
      "Epoch 2/30\n",
      "9s - loss: 2.3952 - acc: 0.7672 - val_loss: 2.0793 - val_acc: 0.8402\n",
      "Epoch 3/30\n",
      "9s - loss: 2.0099 - acc: 0.8194 - val_loss: 1.8396 - val_acc: 0.8344\n",
      "Epoch 4/30\n",
      "10s - loss: 1.8460 - acc: 0.8091 - val_loss: 1.6807 - val_acc: 0.8368\n",
      "Epoch 5/30\n",
      "10s - loss: 1.7011 - acc: 0.7964 - val_loss: 1.5064 - val_acc: 0.8440\n",
      "Epoch 6/30\n",
      "11s - loss: 1.4831 - acc: 0.8300 - val_loss: 1.3573 - val_acc: 0.8512\n",
      "Epoch 7/30\n",
      "9s - loss: 1.3055 - acc: 0.8433 - val_loss: 1.2686 - val_acc: 0.8192\n",
      "Epoch 8/30\n",
      "9s - loss: 1.1803 - acc: 0.8407 - val_loss: 1.4287 - val_acc: 0.5196\n",
      "Epoch 9/30\n",
      "10s - loss: 1.2316 - acc: 0.8080 - val_loss: 1.0854 - val_acc: 0.8480\n",
      "Epoch 10/30\n",
      "12s - loss: 1.0236 - acc: 0.8473 - val_loss: 0.9340 - val_acc: 0.8508\n",
      "Epoch 11/30\n",
      "11s - loss: 0.9122 - acc: 0.8502 - val_loss: 0.8458 - val_acc: 0.8532\n",
      "Epoch 12/30\n",
      "11s - loss: 1.2041 - acc: 0.6255 - val_loss: 1.3343 - val_acc: 0.5062\n",
      "Epoch 13/30\n",
      "11s - loss: 1.2800 - acc: 0.5054 - val_loss: 1.2176 - val_acc: 0.4938\n",
      "Epoch 14/30\n",
      "10s - loss: 1.1755 - acc: 0.5014 - val_loss: 1.1259 - val_acc: 0.4938\n",
      "Epoch 15/30\n",
      "9s - loss: 1.0905 - acc: 0.4998 - val_loss: 1.0503 - val_acc: 0.4938\n",
      "Epoch 16/30\n",
      "9s - loss: 1.0180 - acc: 0.5102 - val_loss: 0.9863 - val_acc: 0.4938\n",
      "Epoch 17/30\n",
      "9s - loss: 0.9591 - acc: 0.5077 - val_loss: 0.9303 - val_acc: 0.5062\n",
      "Epoch 18/30\n",
      "10s - loss: 0.9082 - acc: 0.5228 - val_loss: 0.8841 - val_acc: 0.5062\n",
      "Epoch 19/30\n",
      "10s - loss: 0.8085 - acc: 0.6574 - val_loss: 0.6546 - val_acc: 0.8334\n",
      "Epoch 20/30\n",
      "10s - loss: 0.5959 - acc: 0.8465 - val_loss: 0.5486 - val_acc: 0.8572\n",
      "Epoch 21/30\n",
      "9s - loss: 0.5260 - acc: 0.8557 - val_loss: 0.4898 - val_acc: 0.8612\n",
      "Epoch 22/30\n",
      "9s - loss: 0.5029 - acc: 0.8574 - val_loss: 0.4746 - val_acc: 0.8534\n",
      "Epoch 23/30\n",
      "12s - loss: 0.4659 - acc: 0.8609 - val_loss: 0.4546 - val_acc: 0.8590\n",
      "Epoch 24/30\n",
      "9s - loss: 0.4439 - acc: 0.8632 - val_loss: 0.4471 - val_acc: 0.8514\n",
      "Epoch 25/30\n",
      "11s - loss: 0.4265 - acc: 0.8630 - val_loss: 0.4266 - val_acc: 0.8534\n",
      "Epoch 26/30\n",
      "9s - loss: 0.4132 - acc: 0.8627 - val_loss: 0.4110 - val_acc: 0.8544\n",
      "Epoch 27/30\n",
      "12s - loss: 0.4039 - acc: 0.8626 - val_loss: 0.4132 - val_acc: 0.8562\n",
      "Epoch 28/30\n",
      "9s - loss: 0.4052 - acc: 0.8638 - val_loss: 0.4097 - val_acc: 0.8518\n",
      "Epoch 29/30\n",
      "9s - loss: 0.3931 - acc: 0.8664 - val_loss: 0.4000 - val_acc: 0.8562\n",
      "Epoch 30/30\n",
      "10s - loss: 0.3817 - acc: 0.8678 - val_loss: 0.3870 - val_acc: 0.8604\n"
     ]
    }
   ],
   "source": [
    "# Run the model. Feel free to experiment with different batch sizes and number of epochs.\n",
    "history = model.fit(x_train, y_train, epochs=30, batch_size=64, verbose=2, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niEgSUriLiCQhhbuISBL6/6vAUPPuwBhfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x153713898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print the training loss and validation loss\n",
    "plt.plot(history.history[\"loss\"], color='b')\n",
    "plt.plot(history.history[\"val_loss\"], color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 6. Evaluating the model\n",
    "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.85576\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune Model\n",
    "<ul>\n",
    "<li>Current Best One</li>\n",
    "    <ul>\n",
    "    <li>Test Accuracy: 0.85576</li>\n",
    "    <li>Architecture: <br> 4 fully-connected hidden layers (nodes: 512, 256, 128, 64) + 1 fully-connected layer for logits (nodes:2)</li>\n",
    "    <li>Hyperparameters: <br> learning rate (1e-3), batch size (64), epoch (30)</li>\n",
    "    <li>Ways to prevent overfitting: <br> L2-regularization (penalty:1e-2) <br> Dropout rate (0.3)\n",
    "    </li>\n",
    "    </ul>\n",
    "<li>First Try</li>\n",
    "    <ul>\n",
    "    <li>Architecture: <br> 6 fully-connected layers (nodes: 2048, 1024, 512, 256, 128, 64) + 1 fully-connected layer for logits</li>\n",
    "    <li>Result</li>\n",
    "        <ul>\n",
    "        <li>It took 30s to run an epoch (not efficient) although it reach 80% validation accuracy at first epoch</li>\n",
    "        </ul>\n",
    "    </ul>\n",
    "<li>How to tune it to the current one</li>\n",
    "    <ul>\n",
    "        <li>Try to modify the first layer (nodes: 2048, 1024 or 512)\n",
    "        <br>It turns out that node-512 is a good choice considering the efficency\n",
    "        </li>\n",
    "        <li>Add dropout rate\n",
    "        <br>At first, I try with rate 0.5. Then tune to 0.1, 0.3 if the model starts underfitting. Tune back to 0.5 or even 0.7 if overfitting.\n",
    "        </li>\n",
    "        <li>Add L2-Regularization\n",
    "        <br>Start with 1e-4. If model starts overfitting, increase to 1e-2. If the model starts underfitting, consider tune back to 1e-4 or even 1e-5.\n",
    "        </li>\n",
    "        <li>How about Optimizer:\n",
    "        <br>In this case, I also tried Adagrad and SGD, but it seemed that Adam (with default parameters) always did well in what I've tried.\n",
    "        </li>\n",
    "        <li>How about other hyper-parameters?\n",
    "        <br>At some architectures, it only needs 10 epochs to get validation accuracy nearly 85%. After adding L2-regularization and increase dropout rate, it would take around 20% to get a stable validation accuracy 85%.\n",
    "        <br>Set batch size to 128 or 64 can get good result at first few epochs. When choose to use batch size 64, the accuracy would bounce drastically in some cases.\n",
    "        </li>\n",
    "        <li>By the way, due to random initialization of the weights, it may get different resuls when using same paramters</li>\n",
    "    </ul>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
